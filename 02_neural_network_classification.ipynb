{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOI3oMYVxOTcaI2XCYEpXO/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KonradGonrad/PyTorch-deep-learning/blob/main/02_neural_network_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02. Neural Network classification with PyTorch\n",
        "\n",
        "Classification is a problem of predicting whether something is one thing or another (sometimes there can be multiple things as another)\n",
        "\n",
        "links:\n",
        "- https://www.learnpytorch.io/02_pytorch_classification/\n",
        "- https://github.com/mrdbourke/pytorch-deep-learning"
      ],
      "metadata": {
        "id": "B0_J-UTL8VH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Make classification data and get it ready"
      ],
      "metadata": {
        "id": "SNibmBr6piVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "f71ehfFoq-8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles"
      ],
      "metadata": {
        "id": "AKUxKd4Zq6qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a 1000 samples\n",
        "n_samples = 1000\n",
        "\n",
        "# Create circles\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise=0.03,\n",
        "                    random_state = 42)"
      ],
      "metadata": {
        "id": "XNzS4_uerDPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Features: {len(X)}, Labels: {len(y)}\")"
      ],
      "metadata": {
        "id": "koGO5TzrrQ7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First 5 samples: \\n{X[:5]} \\n{y[:5]}\")"
      ],
      "metadata": {
        "id": "odWcu67MrR6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make DataFrame of circle data\n",
        "import pandas as pd\n",
        "circles = pd.DataFrame({\"X1\": X[:, 0],\n",
        "                        \"X2\": X[:, 1],\n",
        "                        \"label\": y})\n",
        "circles"
      ],
      "metadata": {
        "id": "HG3Hzzu8rl0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize, Visualize, Visuzalize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(x=X[:, 0],\n",
        "            y=X[:, 1],\n",
        "            c = y,\n",
        "            cmap=plt.cm.RdYlBu)"
      ],
      "metadata": {
        "id": "hiu7eDOkulQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The data we're working with is often reffered to as a toy dataset, a dataset that is small enought to experiment by still sizeable to enought to practise the fundametals"
      ],
      "metadata": {
        "id": "_A_K2i3VyA6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Check input and output shapes"
      ],
      "metadata": {
        "id": "PcTbGFn3xwpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "y58KgJCbyrSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first example of features and labels\n",
        "X_sample = X[0]\n",
        "y_sample = y[0]\n",
        "\n",
        "print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\n",
        "print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")"
      ],
      "metadata": {
        "id": "a7-TU-lIyxHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 turn data into tensors and create train and test splits"
      ],
      "metadata": {
        "id": "oGP42ndbyyFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "HfUrDcoLmuGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.dtype"
      ],
      "metadata": {
        "id": "CJkB8HPWnDJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn data into tensors\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y= torch.from_numpy(y).type(torch.float)"
      ],
      "metadata": {
        "id": "lU_V6efrmmnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:5], y[:5]"
      ],
      "metadata": {
        "id": "mpx1doXBmqMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2, # test size 20%\n",
        "                                                    train_size=0.8, # train size 80%\n",
        "                                                    random_state = 42\n",
        "                                                    )"
      ],
      "metadata": {
        "id": "4g5y615AnCC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "id": "H3JgPjeiojEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building a model\n",
        "\n",
        "Let's build a model to classify our blue and red dots\n",
        "\n",
        "To do so, we want to:\n",
        "1. Setup device agnostic code so our code will run faster on GPU if there is one\n",
        "2. Construct a model (by subclassing 'nn.Module)\n",
        "3. Define a loss function and optimizer\n",
        "4. Create a training and test loop\n",
        "\n",
        "* For better visualization of model working, this site is very usefull:\n",
        "https://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=20&dataset=circle&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=5&networkShape=8,8&seed=0.29212&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
      ],
      "metadata": {
        "id": "lT5O73dRon_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# device agnostic code\n",
        "import torch\n",
        "\n",
        "DEVICE_DESTINATION = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "tKR-rgXUoWsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've setup device agnostic code, let's create a model that:\n",
        "\n",
        "1. Subclasses 'nn.Module' (Almost all models in PyTorch subclass 'nn.Module')\n",
        "2. Create 2 'nn.Linear()' layers that are capable of handling the shapes of our data\n",
        "3. Defubes a \"forward()\" method that outlines the forward pass (or forward computation) of the model\n",
        "4. Instatite an instance of our model class and send it to the traget \"device\""
      ],
      "metadata": {
        "id": "CZ8cWqwOog_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape # Parameters, where are 800 rows with two elements as Parameters\n",
        "y_train.shape # Outputs, where are 800 rows with one element as Output"
      ],
      "metadata": {
        "id": "IADe6ciaq5c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "# 1. Construct a model that subclasses nn.Module\n",
        "class CircleModelV0(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # 2. Create two nn.Linear layers capable of handling the shapes of our data\n",
        "    self.layer_1 = nn.Linear(in_features=2, out_features=8) # Takes in 2 features and upscales to 5 features\n",
        "    self.layer_2 = nn.Linear(in_features=8, out_features=1) # Takes in 5 features and gives 1 output, it's the output layer\n",
        "\n",
        "\n",
        "  # 3. Create a forward() method that outlines the forward pass\n",
        "  def forward(self, x):\n",
        "    return self.layer_2(self.layer_1(x)) # x -> layer_1 -> layer_2 -> output\n",
        "\n",
        "# 4. Instantitate an instane of our model class and send it to the target device\n",
        "model_0 = CircleModelV0().to(DEVICE_DESTINATION)\n",
        "model_0"
      ],
      "metadata": {
        "id": "0X_QNfyLqbCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(model_0.parameters()).device"
      ],
      "metadata": {
        "id": "rjzo3XwjsenD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_01 = torch.nn.Sequential(\n",
        "    nn.Linear(in_features = 2, out_features=5),\n",
        "    nn.Linear(in_features=5, out_features = 1)\n",
        ").to(DEVICE_DESTINATION)"
      ],
      "metadata": {
        "id": "5m7v6tPys-fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model_0: \\n{model_0}\")\n",
        "print(f\"Model_01: \\n{model_01}\")"
      ],
      "metadata": {
        "id": "ZxUpokrJElU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  untrained_preds = model_0(X_test.to(DEVICE_DESTINATION))"
      ],
      "metadata": {
        "id": "V3EUkXN3Ep4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of preds: {len(untrained_preds)} and shape: {untrained_preds.shape}\")\n",
        "print(f\"Length of test samples: {len(y_test)} and shape: {y_test.shape}\")\n",
        "print(f\"First ten of test samples: {y_test[:10]}\")\n",
        "print(f\"first ten of preds: {torch.round(untrained_preds[:10])}\")"
      ],
      "metadata": {
        "id": "7Mh1ylExGrmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Setup loss function and optimizer\n",
        "\n",
        "Which loss function or optimizer should you use?\n",
        "\n",
        "Again... this is problem specific.\n",
        "\n",
        "For example for regression you might want MAE or MSE\n",
        "\n",
        "For classification you might want binary cross entropy or categorical cross entropy (cross entropy)\n",
        "\n",
        "And for optimizers, two of the most coomon and useful are SGD and Adam, however PyTorch have more other optimizers\n",
        "\n",
        "* For the loss function we're going to use 'torch.nn.BECWithLogistsLoss()', for more on what binary cross entropy is, check out this article:\n",
        "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
        "* For a definition on what a logit is in deep learning:\n",
        "https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean"
      ],
      "metadata": {
        "id": "6-xOaNHnHim0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the loss function\n",
        "# loss_fn = nn.BCELoss() # BCELoss = requires inputs to have gone through the sigmoid activation function prior to input to BCELoss\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid activation function built-in, what means the output activation\n",
        "\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr = 0.1)"
      ],
      "metadata": {
        "id": "OEhGR8cjm3pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy - out of 100 examples, what percantage does our model get right\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct/len(y_pred)) * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "_rHS6zzAoVjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train model\n",
        "\n",
        "to train our model, we're going to need to build a training loop with the following steps:\n",
        "\n",
        "1. Forward pass\n",
        "2. Calculate the loss\n",
        "3. Optimizer zero graz\n",
        "4. Loss backward (backpropagation)\n",
        "5. Optimizer step (gradien descent)"
      ],
      "metadata": {
        "id": "PgkAOFJ3oXCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Going from raw logits -> predictions probabilities -> prediction labels\n",
        "\n",
        "Our model outputs are going to be raw **logits**.\n",
        "\n",
        "We can convert these **logits** into **prediction probabilities** by passing them to some kind of activation function (e.g. sigmoid for binary crossentropy and softmax multiclass classification)\n",
        "\n",
        "Then we can convert our model's prediction probabilities to prediction labels by either rounding them or taking the argmax()"
      ],
      "metadata": {
        "id": "8pP8hzuvrHCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 outputs of the forward pass on the test data\n",
        "print(f\"Our model device: {next(model_0.parameters()).device}\")\n",
        "\n",
        "with torch.inference_mode():\n",
        "  y_logits = model_01(X_test.to(DEVICE_DESTINATION))\n",
        "y_logits[:5]"
      ],
      "metadata": {
        "id": "lEjOm1JxrQXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our prediction propability values, we need to perform a range-style rounding on them:\n",
        "* 'y_pred_probs' >= 0.5, 'y=1' (class 1)\n",
        "* 'y_pred_probs' < 0.5, 'y=0' (class 0)"
      ],
      "metadata": {
        "id": "VXEty4V6uj5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the sigmoid activation function on our model logits to turn them into prediction probabilities\n",
        "y_pred_probs = torch.sigmoid(y_logits)\n",
        "torch.round(y_pred_probs[:5])"
      ],
      "metadata": {
        "id": "VeddEsN_rQxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted labels\n",
        "y_preds = torch.round(y_pred_probs)\n",
        "\n",
        "# In full (logits -> pred probs -> pred labels)\n",
        "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(DEVICE_DESTINATION))))\n",
        "\n",
        "# Check for equality\n",
        "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))"
      ],
      "metadata": {
        "id": "e4zqSnGBuJI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 building a training and testing loop"
      ],
      "metadata": {
        "id": "5UWbwDRuvE3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape\n",
        "y_preds.squeeze().shape"
      ],
      "metadata": {
        "id": "jK-f6GIkwjaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Set he number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# Put data to target device\n",
        "X_train, y_train = X_train.to(DEVICE_DESTINATION), y_train.to(DEVICE_DESTINATION)\n",
        "X_test, y_test = X_test.to(DEVICE_DESTINATION), y_test.to(DEVICE_DESTINATION)\n",
        "\n",
        "# Build training and evaluation loop\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_0.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_logits = model_0(X_train).squeeze()\n",
        "  y_pred = torch.round(torch.sigmoid(y_logits)) # Turn logits -> pred probs -> pred labels\n",
        "\n",
        "  # 2. Calculate accuracy/loss\n",
        " # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction # propabilities\n",
        " #                y_train)\n",
        "  train_loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input\n",
        "                 y_train)\n",
        "  train_acc = accuracy_fn(y_true=y_train,\n",
        "                          y_pred=y_pred)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. loss backward\n",
        "  train_loss.backward()\n",
        "\n",
        "  # 5. Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 1. Forward pass\n",
        "    test_logits = model_0(X_test).squeeze()\n",
        "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "    # 2. Calculate accuracy/loss\n",
        "    test_loss = loss_fn(test_logits,\n",
        "                        y_test)\n",
        "    test_acc = accuracy_fn(y_true=y_test,\n",
        "                           y_pred=test_pred)\n",
        "\n",
        "  # Print out what's happening\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"epoch: {epoch} | train_loss: {train_loss:.5f} | train_acc: {train_acc:.2f} | test_loss: {test_loss:.5f} | test_acc: {test_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "TBeZhn8fvwxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions and evaluate the model\n",
        "\n",
        "From the metrics it looks like our model isn't learning anything..\n",
        "\n",
        "So to inspect it let's make some predictions and make them visual\n",
        "\n",
        "In other words, \"Visualize, visualize, visualize\"\n",
        "\n",
        "To do so, we're going to import a function called 'plot_decision_boundry()'\n",
        "\n",
        "To do so, we're going to import a function called 'plot_decision_boundary()' from: https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"
      ],
      "metadata": {
        "id": "bfwaO2yZv2Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if it's not already downloaded)\n",
        "\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"Helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "from helper_functions import plot_predictions, plot_decision_boundary"
      ],
      "metadata": {
        "id": "yOSPN7BOKcph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary of the model\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Train')\n",
        "plot_decision_boundary(model_0, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Test')\n",
        "plot_decision_boundary(model_0, X_test, y_test)"
      ],
      "metadata": {
        "id": "RE-hxbemPxxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Improving a model (from a model perspective)\n",
        "\n",
        "* Add more layers - give the model more chances to learn about patterns in the data\n",
        "* Add more hidden units - go from 5 hidden units to 10 hidden units\n",
        "* Changing the activation functions\n",
        "* Change the optimization function\n",
        "* Change the learning rate\n",
        "* Change the loss function\n",
        "\n",
        "These options are all from a model's perspective beacouse they deal directly with the model, rather than the data\n",
        "\n",
        "And beacouse these options are all values we (as Machine Learning engineers and data scientists) can change, they are reffered as **hyperparameters**\n",
        "\n",
        "Let's try and improve our model by:\n",
        "* Adding more hidden units: 5 -> 10\n",
        "* Increase the number of layers: 2 -> 3\n",
        "* Increase the number of epoch: 100 -> 1000"
      ],
      "metadata": {
        "id": "Yfr2MryJQ5Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CircleModelV1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_1 = nn.Linear(in_features = 2, out_features = 10)\n",
        "    self.layer_2 = nn.Linear(in_features = 10, out_features = 10)\n",
        "    self.layer_3 = nn.Linear(in_features = 10, out_features = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #z = self.layer_1(x)\n",
        "    #z = self.layer_2(z)\n",
        "    #z = self.layer_3(z)\n",
        "    return self.layer_3(self.layer_2(self.layer_1(x))) # This way of writing operations leverages speed ups where posible behind scenes\n",
        "\n",
        "model_1 = CircleModelV1().to(DEVICE_DESTINATION)\n",
        "model_1"
      ],
      "metadata": {
        "id": "4L9nt_6ID54f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a loss function\n",
        "loss_fn_v1 = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer_v1 = torch.optim.SGD(model_1.parameters(),\n",
        "                             lr = 0.01)"
      ],
      "metadata": {
        "id": "LQ_rAG3CPYJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a training and evaluation loop for model_1\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "X_train, y_train = X_train.to(DEVICE_DESTINATION), y_train.to(DEVICE_DESTINATION)\n",
        "X_test, y_test = X_test.to(DEVICE_DESTINATION), y_test.to(DEVICE_DESTINATION)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model_1.train()\n",
        "\n",
        "  y_train_logits = model_1(X_train).squeeze()\n",
        "  y_train_preds = torch.round(torch.sigmoid(y_train_logits))\n",
        "\n",
        "  train_loss = loss_fn_v1(y_train_logits,\n",
        "                          y_train)\n",
        "  train_acc = accuracy_fn(y_true=y_train,\n",
        "                         y_pred=y_train_preds)\n",
        "\n",
        "  optimizer_v1.zero_grad()\n",
        "\n",
        "  train_loss.backward()\n",
        "\n",
        "  optimizer_v1.step()\n",
        "\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    y_test_logits = model_1(X_test).squeeze()\n",
        "    y_test_preds = torch.round(torch.sigmoid(y_test_logits))\n",
        "\n",
        "    test_loss = loss_fn_v1(y_test_logits,\n",
        "                           y_test)\n",
        "    test_acc = accuracy_fn(y_true=y_test,\n",
        "                           y_pred=y_test_preds)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"epoch: {epoch} | train loss: {train_loss:.5f} | train acc: {train_acc:.2f} | test loss: {test_loss:.5f} | test acc: {test_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "bf6SxgS1T_X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Train')\n",
        "plt.subplot(1,2,1)\n",
        "plot_decision_boundary(model_1, X_train, y_train)\n",
        "plt.title('Train')\n",
        "plt.subplot(1,2,2)\n",
        "plot_decision_boundary(model_1, X_test, y_test)"
      ],
      "metadata": {
        "id": "Y1LvoCCaY2Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Preparing data to see if our model can fit a straight line\n",
        "\n",
        "One way to troubleshoot to a larger problem is to test out a smaller problem"
      ],
      "metadata": {
        "id": "0T4sgWnkgsvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data (same as notebook 01)\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.01\n",
        "\n",
        "# Create data\n",
        "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y_regression = weight * X_regression + bias # Linear regression formula\n",
        "\n",
        "# Check the data\n",
        "print(len(X_regression))\n",
        "print(X_regression[:5], y_regression[:5])"
      ],
      "metadata": {
        "id": "qJgtcoMMnvwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test splits\n",
        "train_split = int(0.8 * len(X_regression))\n",
        "X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n",
        "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
        "\n",
        "# Check the lengths of each\n",
        "len(X_train_regression), len(y_train_regression), len(X_test_regression), len(y_test_regression)"
      ],
      "metadata": {
        "id": "B74gOx1boCPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data = X_train_regression,\n",
        "                 train_labels=y_train_regression,\n",
        "                 test_data=X_test_regression,\n",
        "                 test_labels=y_test_regression)"
      ],
      "metadata": {
        "id": "6t6wJJqOru_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1"
      ],
      "metadata": {
        "id": "do8tnN7OsWLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same architecture as model_1 (but using nn.Sequential())\n",
        "model_11 = nn.Sequential(\n",
        "    nn.Linear(in_features = 1, out_features=10),\n",
        "    nn.Linear(in_features = 10, out_features=10),\n",
        "    nn.Linear(in_features = 10, out_features = 1)\n",
        ")\n",
        "model_11"
      ],
      "metadata": {
        "id": "Z95LjFausYwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(model_11.parameters(),\n",
        "                            lr=0.01)"
      ],
      "metadata": {
        "id": "JU0WPJS7usSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "X_train_regression, y_train_regression = X_train_regression.to(DEVICE_DESTINATION), y_train_regression.to(DEVICE_DESTINATION)\n",
        "X_test_regression, y_test_regression = X_test_regression.to(DEVICE_DESTINATION), y_test_regression.to(DEVICE_DESTINATION)\n",
        "model_11.to(DEVICE_DESTINATION)\n",
        "\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "  model_11.train()\n",
        "\n",
        "  y_preds = model_11(X_train_regression)\n",
        "  loss = loss_fn(y_preds, y_train_regression)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model_11.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    test_pred = model_11(X_test_regression)\n",
        "    test_loss = loss_fn(test_pred, y_test_regression)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"epoch: {epoch} | train_loss: {loss:.5f} | test_loss: {test_loss:.5f}\")"
      ],
      "metadata": {
        "id": "CjCfOVm6vO7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn on evaluation mode\n",
        "model_11.eval()\n",
        "\n",
        "# Make predictions\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_11(X_test_regression)\n",
        "\n",
        "# Plot data and predictions\n",
        "plot_predictions(train_data=X_train_regression.cpu(),\n",
        "                 train_labels=y_train_regression.cpu(),\n",
        "                 test_data=X_test_regression.cpu(),\n",
        "                 test_labels=y_test_regression.cpu(),\n",
        "                 predictions=y_preds.cpu())"
      ],
      "metadata": {
        "id": "K-zZdBFzv1Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. The missiing piece: non-linearity\n",
        "\n",
        "\"What patterns could you draw if you were given an infinite amount of a straing and non-straight lines?\"\"\n",
        "\n",
        "Or in machine learning terms, an infinite (but really it is finite) of linear and non-linear functions?"
      ],
      "metadata": {
        "id": "MwxzRSkG2y9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Recreating non-linear data (red and blue circles)"
      ],
      "metadata": {
        "id": "LWa97ct-BA47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and plot data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "n_samples = 1000\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise=0.03,\n",
        "                    random_state=42)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)\n"
      ],
      "metadata": {
        "id": "JMvBlnyFN6PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Turn data into tensors\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, train_size = 0.8, random_state = 42)\n",
        "\n",
        "print(f\"X_train: {len(X_train)} | y_train: {len(y_train)} | X_test: {len(X_test)} | y_test: {len(y_test)}\")"
      ],
      "metadata": {
        "id": "sTY3Px00ORhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Building a model with non-linearity\n",
        "\n",
        "* Linear = straight lines\n",
        "* Non-Linear = non straight lines\n",
        "\n",
        "Artificial neural networks are a large combination of linear (straight) and (non-linear) functions which are potentially able to find patterns in data"
      ],
      "metadata": {
        "id": "9A3Yo6NsOtI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with non-linear activation function\n",
        "from torch import nn\n",
        "\n",
        "class CircleModelV2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = nn.Linear(in_features = 2, out_features=10)\n",
        "    self.layer_2 = nn.Linear(in_features = 10, out_features=10)\n",
        "    self.layer_3 = nn.Linear(in_features = 10, out_features = 1)\n",
        "    self.relu = nn.ReLU() # non-linear activation function\n",
        "    # self.sigmoid == nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
        "\n",
        "model_2 = CircleModelV2().to(DEVICE_DESTINATION)\n",
        "model_2"
      ],
      "metadata": {
        "id": "lnblPRy3QKT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer\n",
        "\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                            lr = 0.1)"
      ],
      "metadata": {
        "id": "7aRvsvItcp9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train loop\n",
        "\n",
        "# Torch random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "X_train, y_train = X_train.to(DEVICE_DESTINATION), y_train.to(DEVICE_DESTINATION)\n",
        "X_test, y_test = X_test.to(DEVICE_DESTINATION), y_test.to(DEVICE_DESTINATION)\n",
        "\n",
        "epochs = 2000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  ## Train time\n",
        "  model_2.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_train_logits = model_2(X_train).squeeze()\n",
        "  y_train_preds = torch.round(torch.sigmoid(y_train_logits))\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "\n",
        "  train_loss = loss_fn(y_train_logits, y_train)\n",
        "  train_acc = accuracy_fn(y_train, y_train_preds)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward\n",
        "  train_loss.backward()\n",
        "\n",
        "  # 5. Optimizer step step step\n",
        "  optimizer.step()\n",
        "\n",
        "  ## Test time\n",
        "  # Model evaluation\n",
        "  model_2.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    # Forward pass\n",
        "    y_test_logits = model_2(X_test).squeeze()\n",
        "    y_test_preds = torch.round(torch.sigmoid(y_test_logits))\n",
        "\n",
        "    # Calculate the loss\n",
        "    test_loss = loss_fn(y_test_logits, y_test)\n",
        "    test_acc = accuracy_fn(y_test, y_test_preds)\n",
        "\n",
        "  ## Print out what's happening\n",
        "  if epoch % 200 == 0:\n",
        "    print(f\"epoch: {epoch} | train_loss: {train_loss:.5f} | train_acc: {train_acc:.2f}% | test_loss: {test_loss:.5f} | test_acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "VoLtTEHMR3k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize, visualize, visualize\n",
        "## Train and test data\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_2, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_2, X_test, y_test)"
      ],
      "metadata": {
        "id": "fp6pfcXCdViW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Linear model\")\n",
        "plot_decision_boundary(model_1, X_train, y_train) # Linear\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Non-Linear model\")\n",
        "plot_decision_boundary(model_2, X_train, y_train) # Non-Linear"
      ],
      "metadata": {
        "id": "W8M5lRoQjHNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Replicating non-linear activation functions\n",
        "\n",
        "Neural networks, rather than us telling the model what to learn, we give it the tools to discover patterns in data and it tries to figure out patterns on its own\n",
        "\n",
        "Abd these tools are linear & non-linear functions"
      ],
      "metadata": {
        "id": "NJupGmqMjTtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "A = torch.arange(-10, 10, 1, dtype=torch.float)\n",
        "A.dtype"
      ],
      "metadata": {
        "id": "ogzuQX5Cp8F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the data\n",
        "plt.plot(A)"
      ],
      "metadata": {
        "id": "C8bk-yAlqDkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.relu(A))"
      ],
      "metadata": {
        "id": "b4rz_64AqTqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_fn(x: torch.Tensor) -> torch.Tensor:\n",
        "  return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n",
        "\n",
        "print(f\"function for -5 is equal {relu_fn(torch.tensor(-5))}\")\n",
        "print(f\"function for 5 is equal {relu_fn(torch.tensor(5))}\")\n",
        "print(f\"Function for our tensor A:\\n{A} \\nIs equal: \\n{relu_fn(A)}\")"
      ],
      "metadata": {
        "id": "rovXSQR3qcgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot reLU activation function\n",
        "plt.figure(figsize=(12, 6))\n",
        "# first visualization\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Custom relu function\")\n",
        "plt.plot(relu_fn(A))\n",
        "# Second visualization\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Original relu function by PyTorch\")\n",
        "plt.plot(torch.relu(A))"
      ],
      "metadata": {
        "id": "Uf7dyQvsqtw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do the same for sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + torch.exp(-x))"
      ],
      "metadata": {
        "id": "StktA4Zwr1Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid comparision\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Custom sigmoid function\")\n",
        "plt.plot(sigmoid(A))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Torch sigmoid function\")\n",
        "plt.plot(torch.sigmoid(A))"
      ],
      "metadata": {
        "id": "4KN3JHo_3Tzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WgagnNUh3aQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}